# LLM Serving Papers
**Last Update: 2024-09-04**
- [LLM Serving Papers](#llm-serving-papers)
  - [Survey](#survey)

## Survey
|Paper|Conference|Category|Abstract
|--|--|--|--|
[[2407.12391] LLM Inference Serving: Survey of Recent Advances and Opportunities (arxiv.org)](https://arxiv.org/abs/2407.12391)|Arxiv||focus on system-level enhancements without altering LLM decoding algorithms, including memory (KV cache management), compute (scheduling), scalability (cloud-LLM) and emerging fields|
[[2407.14645] Performance Modeling and Workload Analysis of Distributed Large Language Model Training and Inference (arxiv.org)](https://arxiv.org/abs/2407.14645)|Arxiv|||
[[2404.14294] A Survey on Efficient Inference for Large Language Models (arxiv.org)](https://arxiv.org/abs/2404.14294)|Arxiv|||
[The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving](https://arxiv.org/pdf/2405.11299)|Arxiv|Long Context||
