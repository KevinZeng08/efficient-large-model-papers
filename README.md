# LLM Serving Papers
**Last Update: 2024-09-06**
- [LLM Serving Papers](#llm-serving-papers)
  - [Survey](#survey)

## Survey
|Paper|Code|Category|Abstract
|--|--|--|--|
[[2407.12391] LLM Inference Serving: Survey of Recent Advances and Opportunities (arxiv.org)](https://arxiv.org/abs/2407.12391)|||Focus on system-level enhancements without altering LLM decoding algorithms, including memory (KV cache management), compute (scheduling), scalability (cloud-LLM) and emerging fields|
[[2407.14645] Performance Modeling and Workload Analysis of Distributed Large Language Model Training and Inference (arxiv.org)](https://arxiv.org/abs/2407.14645)||||
[[2404.14294] A Survey on Efficient Inference for Large Language Models (arxiv.org)](https://arxiv.org/abs/2404.14294)||||
[[2402.16363] LLM Inference Unveiled: Survey and Roofline Model Insights (arxiv.org)](https://arxiv.org/abs/2402.16363)|https://github.com/hahnyuan/LLM-Viewer||Framework based on Roofline model, profiling different hardware and inference configs. Cover model compression, algorithm, system and hardware optimizations|
[The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving](https://arxiv.org/pdf/2405.11299)||Long Context||

